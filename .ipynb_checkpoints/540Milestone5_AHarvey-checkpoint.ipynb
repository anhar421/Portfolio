{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File dsc540_mlestatus.csv does not exist: 'dsc540_mlestatus.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2775f6c7a444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import previously created datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmle_status\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dsc540_mlestatus.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mgbif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dsc540_gbif.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File dsc540_mlestatus.csv does not exist: 'dsc540_mlestatus.csv'"
     ]
    }
   ],
   "source": [
    "# Import previously created datasets\n",
    "mle_status = pd.read_csv('dsc540_mlestatus.csv')\n",
    "gbif = pd.read_csv('dsc540_gbif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif.head()\n",
    "# I'm realizing now that I need to change the column header for 'species' to 'scientific_name' since that is my column in common\n",
    "# between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif = gbif.rename(columns = {'species' : 'scientific_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to SQL. I found this on the SQLite docs. It was surprisingly easy! I'm starting to understand the application of SQL\n",
    "# better although some of the larger concepts are still confusing.\n",
    "con = sql.connect('aza_data')\n",
    "mle_status.to_sql('mle_status', con=con)\n",
    "gbif.to_sql('gbif', con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first iteration I did of this code, I had 'FROM gbif JOIN mle_status...'. I didn't like the order that the columns were in.\n",
    "# I was going to manually rearrange them but I realized that they were probably in that order because of the order of commands I\n",
    "# put in for the join. Switching which table was called first solved that problem exactly.\n",
    "#\n",
    "# I only needed to do one join because I had joined the original AZA list of animals with the Endangered Status during Milestone 3.\n",
    "\n",
    "with sql.connect('aza_data') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    aza_df = pd.DataFrame(cursor.execute('SELECT* FROM mle_status JOIN gbif ON mle_status.scientific_name = gbif.scientific_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aza_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first and seventh columns with the index numbers is unneccesary, as is the duplicate column of scientific_name in column 9.\n",
    "#  Also, I guess because of the join method it doesn't matter that I changed the column names. It didn't merge the scientific_name\n",
    "# columns together. I think that would be something that happened with a different join, but this works.\n",
    "aza_df = aza_df.drop([0, 7, 9], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aza_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to rename the column headers in order to work with the data easier.\n",
    "columns = {1: 'common_name', 2: 'scientific_name', 3: 'MLE_combined', 4: 'MLE_male', 5: 'MLE_female', 6: 'status', 8: 'occurrenceStatus',\n",
    "           10: 'country', 11: 'locality', 12: 'year', 13: 'month', 14: 'day'}\n",
    "aza_df = aza_df.rename(columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aza_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After discovering the discrepencies in my other table, I decided to convert the MLE data here as well.\n",
    "aza_df = aza_df.replace('-', np.nan)\n",
    "aza_df = aza_df.replace('None', np.nan)\n",
    "aza_df = aza_df.replace('See Full Repo', np.nan)\n",
    "\n",
    "aza_df['MLE_combined'] = aza_df['MLE_combined'].astype(float)\n",
    "aza_df['MLE_male'] = aza_df['MLE_male'].astype(float)\n",
    "aza_df['MLE_female'] = aza_df['MLE_female'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to see how many total rows are in the new dataframe so I used shape.\n",
    "aza_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table has the unique information for each species regarding MLE (median life expectancy) and Endangered Species Status.\n",
    "# There is one row per species versus the joined table which has one row per occurrence. \n",
    "\n",
    "with sql.connect('aza_data') as conn:\n",
    "    cursor = conn.cursor()\n",
    "    mlestat_df = pd.DataFrame(cursor.execute('SELECT* FROM mle_status'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean up the dataframe like I did with the joined one.\n",
    "mlestat_df = mlestat_df.drop([0], axis=1)\n",
    "col2 = {1: 'common_name', 2: 'scientific_name', 3: 'MLE_combined', 4: 'MLE_male', 5: 'MLE_female', 6: 'status'}\n",
    "mlestat_df = mlestat_df.rename(col2, axis=1)\n",
    "mlestat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know why I didn't replace the '-' in this data with NaN earlier. I also don't know why SQL changed the 'NaNs' that were\n",
    "# there to 'None' (see first row).\n",
    "\n",
    "mlestat_df = mlestat_df.replace('-', np.nan)\n",
    "mlestat_df = mlestat_df.replace('None', np.nan)\n",
    "mlestat_df = mlestat_df.replace('See Full Repo', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was getting errors that indicated that the numbers in the MLE columns may have been written as strings. So I converted them\n",
    "# all to floats.\n",
    "mlestat_df['MLE_combined'] = mlestat_df['MLE_combined'].astype(float)\n",
    "mlestat_df['MLE_male'] = mlestat_df['MLE_male'].astype(float)\n",
    "mlestat_df['MLE_female'] = mlestat_df['MLE_female'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to subset each category of Endangered Status into smaller dataframes so that I can plot histograms of MLE frequency\n",
    "# according to each type.\n",
    "least = mlestat_df[mlestat_df.status == 'Least Concern']\n",
    "threat = mlestat_df[mlestat_df.status == 'Threatened']\n",
    "endangered = mlestat_df[mlestat_df.status == 'Endangered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to show the MLE compared to the endangered status across all species. I played around with a few different attempts \n",
    "# before finding some good documentation to walk me through creating multiple density curves.\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sb.distplot(least['MLE_female'], hist = False, kde = True, label='Least Concern', color = 'blue')\n",
    "sb.distplot(threat['MLE_female'], hist = False, kde = True, label='Threatened', color = 'green')\n",
    "sb.distplot(endangered['MLE_female'], hist = False, kde = True, label='Endangered', color = 'red')\n",
    "\n",
    "plt.xlabel('MLE (female) in years', fontsize = 14)\n",
    "plt.ylabel('Species Density', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.title('Distribution of Median Life Expectancy (female) by Endangered Status', fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sb.distplot(least['MLE_male'], hist = False, kde = True, label='Least Concern', color = 'blue')\n",
    "sb.distplot(threat['MLE_male'], hist = False, kde = True, label='Threatened', color = 'green')\n",
    "sb.distplot(endangered['MLE_male'], hist = False, kde = True, label='Endangered', color = 'red')\n",
    "\n",
    "plt.xlabel('MLE (male) in years', fontsize = 14)\n",
    "plt.ylabel('Species Density', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.title('Distribution of Median Life Expectancy (male) by Endangered Status', fontsize = 16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sb.distplot(least['MLE_combined'], hist = False, kde = True, label='Least Concern', color = 'blue')\n",
    "sb.distplot(threat['MLE_combined'], hist = False, kde = True, label='Threatened', color = 'green')\n",
    "sb.distplot(endangered['MLE_combined'], hist = False, kde = True, label='Endangered', color = 'red')\n",
    "\n",
    "plt.xlabel('MLE (combined) in years', fontsize = 14)\n",
    "plt.ylabel('Species Density', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.title('Distribution of Median Life Expectancy (combined) by Endangered Status', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to look at the number of times each animal has a reported appearance in the database. So, I made a list of the unique\n",
    "# species names then took the length of the column (aka the number of rows) for each species name. Then I turned the two lists\n",
    "# into a new dataframe.\n",
    "\n",
    "unique_names = mlestat_df.scientific_name.unique()\n",
    "\n",
    "lengths = []\n",
    "\n",
    "for name in unique_names:\n",
    "    lengths.append(len(aza_df[aza_df['scientific_name'].str.contains(name)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur = pd.DataFrame({'scientific_name' : unique_names, 'num_occur' : lengths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to do a pie chart to see how many times in the total each species occurs, but there are too many different species.\n",
    "# I made a histogram instead, which shows the frequency of the number of occurrences. \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(occur['num_occur'], bins=50)\n",
    "plt.xlabel('# of Occurrences', fontsize = 14)\n",
    "plt.ylabel('# of Species', fontsize = 14)\n",
    "plt.title('Distribution of Total Occurrences for Individual Species', fontsize = 16)\n",
    "plt.xlim(-2, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 371 different species in my final dataframe. I wanted to compare the number of occurrences of all 371 species but since the total number of occurrences is 5855 across 371 species, that is too much for a clean visual. Instead, I'm going to act as if I'm comparing specific individual species to others. The comparison I want to look at will be the seven species of lemur. I want to compare how many occurrences were witnessed for each of these species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to look at the number of occurrences for each of the seven species of lemurs.\n",
    "# Create subset dataframe with just the information for the lemur species.\n",
    "\n",
    "names = ('Varecia variegata', 'Eulemur collaris', 'Eulemur coronatus', 'Eulemur mongoz', 'Microcebus murinus', 'Varecia rubra', \n",
    "         'Lemur catta')\n",
    "lemur = aza_df[aza_df['scientific_name'].isin(names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This figure shows the number of occurrences for each species of lemur reported to GBIF.\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "lemur['scientific_name'].value_counts().plot(kind='barh', color='#bb3f3f')\n",
    "plt.xlabel('Number of occurrences', fontsize = 12)\n",
    "plt.title('Number of GBIF reported occurrences - Lemurs', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to do a scatter plot showing in what years each lemur species identification occurred. I tried to do it with\n",
    "# just matplotlib first, but I needed seaborn for the jitter.\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sb.set(style=\"white\", color_codes=True)\n",
    "sb.stripplot(x='year', y='scientific_name', data=lemur, jitter=True, alpha=0.6, size=10)\n",
    "sb.despine()\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('')\n",
    "plt.title('Reported Occurrences of Lemur Species by Year', fontsize=16)\n",
    "plt.xlim(1825, 2020)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I am going to do what I tried to do before with the larger dataset and make a pie chart showing the distribution of \n",
    "# occurrences for the lemurs. Our previous exercises provided helpful guidance for this step.\n",
    "lnames = lemur.scientific_name.unique()\n",
    "\n",
    "lem_len = []\n",
    "\n",
    "for name in lnames:\n",
    "    lem_len.append(len(lemur[lemur['scientific_name'].str.contains(name)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_occur = pd.DataFrame({'scientific_name' : lnames, 'num_occur' : lem_len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = lem_occur['scientific_name']\n",
    "\n",
    "numbers = lem_occur['num_occur']\n",
    "fig, ax1 = plt.subplots(figsize = (24,12))\n",
    "ax1.axis('equal')\n",
    "wedges, texts, autotexts = ax1.pie(numbers, autopct='%.1f%%')\n",
    "\n",
    "ax1.legend(wedges,\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=16)\n",
    "ax1.legend(labels, loc = 'upper right', fontsize = 18) \n",
    "plt.title('Lemur species - % of total occurrences', fontsize = 28)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was incredibly helpful and educational for me. Although some of the concepts presented had been brought up in past courses, we went into more depth and I was able to better understand the application of tools such as matplotlib and SQL. This process has also taught me that I enjoy the data wrangling part of data science more than some of the other aspects, such as building algorithms. I am detail-oriented by nature so it makes sense to me to go through data with a fine-toothed comb to pull out relevant information and make sure the data is clean.\n",
    "\n",
    "The most challenging part of this project was figuring out what to do with my data. It is one thing to go through homework exercises that tell you what to look for and the types of visualizations to create. It is a whole different set of hurdles to figure those things out for yourself. I tried to keep some end goals in mind to help direct me (such as wanting to see if there was any relationship between the Endangered Species Status and the number of occurrences reported in the GBIF database). Despite how long it took me to accomplish parts of the project, I enjoyed the process. Looking at my data here at its final stage, I can see where, if this were an actual project for an organization, I would need to go back and redo some of my data collection.\n",
    "\n",
    "SQL, in particular, was a brand-new concept for me. Conceptually, I still have a slight gap in my knowledge for the utilization of SQL in a real-world scenario. However, the process of using it makes much more sense to me now. Going forward, I would like to find more resources to help me better understand SQL as well as how to choose visualizations. I would also like to develop my visualization techniques in more areas, such as Tableau/PowerBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
